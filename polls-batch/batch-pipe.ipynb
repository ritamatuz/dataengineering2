{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7164e8-af7b-498d-9607-a4a997498c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet bs4\n",
    "!pip install --quiet selenium\n",
    "!pip install --quiet openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73b462d6-b2cf-4f30-9480-27abcd11fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit, from_csv\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType, IntegerType, DateType\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7300bc8a-130a-429b-a265-de87302b9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    options.add_argument(\"--window-size=1920,1200\")\n",
    "    driver = webdriver.Remote(\"http://selenium:4444/wd/hub\", options=options)\n",
    "    driver.get(\"https://peilingwijzer.tomlouwerse.nl/p/laatste-cijfers.html\")\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    driver.quit()\n",
    "    \n",
    "    publications = soup.find(id=\"PublicationInfo\").find(\"div\")\n",
    "    publication_data = re.search(\"start veldwerk: (.*), einde veldwerk\", str(pub))\n",
    "    publication_date = publication_data.group(1) + \"-2021\"\n",
    "    publication_date_obj = datetime.strptime(publication_date, \"%d-%m-%Y\")\n",
    "    publication_date = publication_date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    downloads = soup.find(id=\"downloads\")\n",
    "    find_links = downloads.find_all(\n",
    "        \"a\", string=\"hier te downloaden (Excel-formaat)\")\n",
    "    download_link = find_links[0][\"href\"]\n",
    "\n",
    "    return {\n",
    "        \"download_link\": download_link, \n",
    "        \"publication_date\": publication_date\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d645854-7945-4138-a1d3-67b3525588dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_pipe(download_link, publication_date):\n",
    "    pDF = pd.read_excel(download_link)\n",
    "    \n",
    "    window = {\n",
    "        \"min\": (datetime.strptime(publication_date, \"%Y-%m-%d\") - timedelta(days=7)).strftime(\"%Y-%m-%d\"),\n",
    "        \"max\": datetime.strptime(publication_date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "    \n",
    "    sparkConf = SparkConf()\n",
    "    sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "    sparkConf.setAppName(\"polls-pipeline\")\n",
    "    sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "    sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "    sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "    \n",
    "    spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "    sparkPolls = spark.createDataFrame(pDF)\n",
    "\n",
    "    drop_poll_columns = [\"Datum\", \"Percentage\", \"PercentageLaag\", \"PercentageHoog\", \"ZetelsLaag\", \"ZetelsHoog\"]\n",
    "    sparkPolls = sparkPolls.select([col for col in sparkPolls.columns if col not in drop_poll_columns])\n",
    "    sparkPolls = sparkPolls.withColumnRenamed(\"Partij\", \"party\").withColumnRenamed(\"Zetels\", \"seats\")\n",
    "\n",
    "    sentimentDataSchema = StructType([\n",
    "        StructField(\"party\", StringType(), True),\n",
    "        StructField(\"window_start\", StringType(), True),\n",
    "        StructField(\"window_end\", DateType(), True),\n",
    "        StructField(\"sentiment\", FloatType(), True)])\n",
    "\n",
    "    sparkSentiment_raw = spark.read.format(\"kafka\")\\\n",
    "                              .option(\"kafka.bootstrap.servers\", \"kafka1:9093\")\\\n",
    "                              .option(\"subscribe\", \"avg_sentiment\")\\\n",
    "                              .load()\n",
    "    \n",
    "    sparkSentiment_lines = sparkSentiment_raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "    sparkSentiment_csv = sparkSentiment_lines.select(from_csv(sparkSentiment_lines.value, \n",
    "                                              sentimentDataSchema.simpleString()))\n",
    "    sparkSentiment = sparkSentiment_csv.select(col(\"from_csv(value).*\"))\n",
    "    sparkSentiment = sparkSentiment.where(sparkSentiment.window_end <= window[\"max\"]).where(sparkSentiment.window_end > window[\"min\"])\n",
    "\n",
    "    combined = sparkSentiment.join(sparkPolls, \"party\", how=\"left\")\n",
    "    correlation = combined.stat.corr(\"sentiment\", \"seats\")\n",
    "    \n",
    "    spark.stop()\n",
    "    \n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee907b55-0973-455e-9e33-c62581b307c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32561668092191476\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-b6302329b366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorrelation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_batch_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscraped_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"download_link\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscraped_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"publication_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrelation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m604800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# create a signal handler which would be invoked on receiving SIGINT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mcancelAllJobs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0mCancel\u001b[0m \u001b[0mall\u001b[0m \u001b[0mjobs\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mscheduled\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mare\u001b[0m \u001b[0mrunning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \"\"\"\n\u001b[0;32m-> 1201\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstatusTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    scraped_data = scrape_data()\n",
    "    correlation = run_batch_pipe(scraped_data[\"download_link\"], scraped_data[\"publication_date\"])\n",
    "    print(correlation)\n",
    "    time.sleep(604800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa19ab-7951-4586-8e16-360175cf12c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
