{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8252f0dc-17ea-4d59-9d9a-0e9ab6fac0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_csv(value): struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- user_id: long (nullable = true)\n",
      " |    |-- created_at: float (nullable = true)\n",
      " |    |-- party: string (nullable = true)\n",
      " |    |-- sentiment: float (nullable = true)\n",
      " |    |-- tweet: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- created_at: float (nullable = true)\n",
      " |-- party: string (nullable = true)\n",
      " |-- sentiment: float (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- created_at: float (nullable = true)\n",
      " |-- party: string (nullable = true)\n",
      " |-- sentiment: float (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, avg, concat, lit, from_csv\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType, IntegerType\n",
    "from time import sleep\n",
    "\n",
    "# Configure spark session\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"twitter-streamer\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "\n",
    "# Define the schema of the source data\n",
    "dataSchema = StructType([\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"user_id\", LongType(), True),\n",
    "    StructField(\"created_at\", FloatType(), True),\n",
    "    StructField(\"party\", StringType(), True),\n",
    "    StructField(\"sentiment\", FloatType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n",
    "\n",
    "# Connect to the kafka source input\n",
    "df_raw = spark.readStream.format(\"kafka\")\\\n",
    "              .option(\"kafka.bootstrap.servers\", \"kafka1:9093\")\\\n",
    "              .option(\"subscribe\", \"twitter_politics\")\\\n",
    "              .option(\"startingOffsets\", \"earliest\")\\\n",
    "              .load()\n",
    "\n",
    "# Convert input bytes into string and convert to columns\n",
    "lines = df_raw.selectExpr(\"CAST(value AS STRING)\")\n",
    "df = lines.select(from_csv(lines.value, dataSchema.simpleString()))\n",
    "df.printSchema()\n",
    "\n",
    "df_gs = df.select(col(\"from_csv(value).*\"))\n",
    "df_gs.printSchema()\n",
    "\n",
    "# Create event_time column for use with a Window\n",
    "df_gs = df_gs.selectExpr(\"*\", \"CAST(created_at as timestamp) as event_time\")\n",
    "df_gs.printSchema()\n",
    "\n",
    "# Compute the average score for each party\n",
    "df_results = df_gs.groupBy(window(col(\"event_time\"), \"7 days\"), \"party\")\\\n",
    "                  .agg(avg(\"sentiment\").alias(\"value\"))\n",
    "df_results = df_results.select(\n",
    "    col(\"party\").alias(\"key\"),\n",
    "    concat(col(\"party\"), lit(\",\"), col(\"window\").cast(\"string\"), lit(\",\"), col(\"value\")).cast(\"string\").alias(\"value\"))\n",
    "\n",
    "df_results.printSchema()\n",
    "\n",
    "\n",
    "query = df_results.writeStream.format(\"kafka\")\\\n",
    "                  .option(\"kafka.bootstrap.servers\", \"kafka1:9093\")\\\n",
    "                  .option(\"checkpointLocation\", \"/home/jovyan/checkpoint\")\\\n",
    "                  .option(\"topic\", \"avg_sentiment\")\\\n",
    "                  .outputMode(\"complete\")\\\n",
    "                  .start()\n",
    "\n",
    "#Write the output to a stream\n",
    "# df_query = df_results.writeStream.queryName(\"debugging\")\\\n",
    "#                      .format(\"memory\")\\\n",
    "#                      .outputMode(\"complete\")\\\n",
    "#                      .start()\n",
    "\n",
    "# try:\n",
    "#     for x in range(10):\n",
    "#         spark.sql(\"SELECT * FROM debugging\").show()\n",
    "#         sleep(10)\n",
    "# except KeyboardInterrupt:\n",
    "#     df_query.stop()\n",
    "#     spark.stop()\n",
    "#     print(\"Stopped the streaming query and spark context\")\n",
    "# finally:\n",
    "#     spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2638b7-27c8-4da9-8615-ebe9e6738e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c67806b-3e62-49d2-ae55-0f3cd72bafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cff2dd-f952-463b-9706-1cfe93395579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}