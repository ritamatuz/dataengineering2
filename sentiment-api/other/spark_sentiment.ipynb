{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c8d466f-4017-4951-a67c-c2c2e5d9e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2aa216cc-65ae-47d1-9aa9-71c0431af5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab7_1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "924f6246-0066-445f-affa-a880013fdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = StructType(\n",
    "    [StructField(\"Tweet\", StringType(), True),\n",
    "     StructField(\"Sentiment\", StringType(), True),\n",
    "     StructField(\"Party\", StringType(), True)\n",
    "     ])\n",
    "\n",
    "# Read from a source \n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1).option(\"delimiter\", \";\") \\\n",
    "    .csv(\"/home/jovyan/data/tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fdad9d9f-d1a3-49c5-bd9a-315237f5750a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthis works:\\ntweet_counts = sdf.groupBy(\"Party\", \"Sentiment\").count()\\nparty_query = tweet_counts.writeStream.queryName(\"party_counts\").outputMode(\"complete\").format(\"memory\").start()\\nfor x in range(3):\\n    spark.sql(\"SELECT * FROM party_counts\").show()\\n    sleep(15)\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this works:\n",
    "tweet_counts = sdf.groupBy(\"Party\", \"Sentiment\").count()\n",
    "party_query = tweet_counts.writeStream.queryName(\"party_counts\").outputMode(\"complete\").format(\"memory\").start()\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM party_counts\").show()\n",
    "    sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4900e26-cb97-4ee6-9a1f-0ea53e91b71d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;\nJoin Inner\n:- Aggregate [Party#1449], [Party#1449, count(1) AS count#1457L]\n:  +- Filter (Sentiment#1448 = positive)\n:     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5a250bef,csv,List(),Some(StructType(StructField(Tweet,StringType,true), StructField(Sentiment,StringType,true), StructField(Party,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, delimiter -> ;, path -> /home/jovyan/data/tweets),None), FileSource[/home/jovyan/data/tweets], [Tweet#1447, Sentiment#1448, Party#1449]\n+- Aggregate [Party#1469], [Party#1469, count(1) AS count#1464L]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5a250bef,csv,List(),Some(StructType(StructField(Tweet,StringType,true), StructField(Sentiment,StringType,true), StructField(Party,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, delimiter -> ;, path -> /home/jovyan/data/tweets),None), FileSource[/home/jovyan/data/tweets], [Tweet#1467, Sentiment#1468, Party#1469]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-4e8ff93f5e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mparty_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bla\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM bla\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Multiple streaming aggregations are not supported with streaming DataFrames/Datasets;\nJoin Inner\n:- Aggregate [Party#1449], [Party#1449, count(1) AS count#1457L]\n:  +- Filter (Sentiment#1448 = positive)\n:     +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5a250bef,csv,List(),Some(StructType(StructField(Tweet,StringType,true), StructField(Sentiment,StringType,true), StructField(Party,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, delimiter -> ;, path -> /home/jovyan/data/tweets),None), FileSource[/home/jovyan/data/tweets], [Tweet#1447, Sentiment#1448, Party#1449]\n+- Aggregate [Party#1469], [Party#1469, count(1) AS count#1464L]\n   +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@5a250bef,csv,List(),Some(StructType(StructField(Tweet,StringType,true), StructField(Sentiment,StringType,true), StructField(Party,StringType,true))),List(),None,Map(maxFilesPerTrigger -> 1, delimiter -> ;, path -> /home/jovyan/data/tweets),None), FileSource[/home/jovyan/data/tweets], [Tweet#1467, Sentiment#1468, Party#1469]\n"
     ]
    }
   ],
   "source": [
    "# this does not work\n",
    "positive_df = sdf.where(sdf.Sentiment == \"positive\")\n",
    "positive_counts = positive_df.groupBy(\"Party\").count()\n",
    "\n",
    "all_counts = sdf.groupBy(\"Party\").count()\n",
    "\n",
    "df = positive_counts.join(all_counts)\n",
    "\n",
    "party_query = df.writeStream.queryName(\"bla\").outputMode(\"complete\").format(\"memory\").start()\n",
    "for x in range(3):\n",
    "    spark.sql(\"SELECT * FROM bla\").show()\n",
    "    sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9c0431de-60cb-442a-bc64-b83dde0e6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
